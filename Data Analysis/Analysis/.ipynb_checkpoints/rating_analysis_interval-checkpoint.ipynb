{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74504646-47ac-4592-9e03-87b5d63e7863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93a19d-255e-4494-af7f-7d6c76713aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a214b-7674-47a0-9c7a-039d9250220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/Processed/rating_numeric.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2794b-5c77-4a4f-828b-8d10eed24acd",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2997a-35e9-4e91-ab8c-584f486d784d",
   "metadata": {},
   "source": [
    "First, we load the data from a CSV file and convert the `start_time` and `end_time` columns to the datetime format. We also drop the `appraisal` column since it is not numeric and won't be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e52406-88de-4a5c-ac22-feb75764a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(file_path, header=0, index_col=[0, 1])\n",
    "\n",
    "# Convert start_time and end_time to datetime format\n",
    "data['start_time'] = pd.to_datetime(data['start_time'])\n",
    "data['end_time'] = pd.to_datetime(data['end_time'])\n",
    "\n",
    "# Drop the 'appraisal' column as it is non-numeric\n",
    "data = data.drop(columns=['appraisal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f30b6c-abe3-4b43-b54f-e204f2e28efb",
   "metadata": {},
   "source": [
    "### Display Basic Information\n",
    "\n",
    "We display basic information about the dataset using the `info()` method. This provides an overview of the dataset, including the number of entries, column names, non-null counts, and data types for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcd192-2a99-4aa3-85ba-897d1e0d6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2331d19-ed02-4c20-9e14-51760cf6bfe3",
   "metadata": {},
   "source": [
    "We use the `describe()` method to generate a statistical summary of all numeric columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a7fc9-a804-4022-af95-831f907de54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc2ed1-5f0c-404a-9cd9-f527272f6f39",
   "metadata": {},
   "source": [
    "And we print the data to inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd7a16-71ac-4d4d-b6e2-0aec7d184de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cbae66-795f-4833-b4dd-9f85db2118cc",
   "metadata": {},
   "source": [
    "Next, we define the lists of independent and dependent variables. These will be used later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fff7e8-2774-40a8-aa67-d8d9ac75bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the emotion intensities columns\n",
    "intensity_columns = [\n",
    "    'joy_intensity', 'sadness_intensity', 'anger_intensity', \n",
    "    'fear_intensity', 'disgust_intensity', 'surprise_intensity'\n",
    "]\n",
    "\n",
    "# Define the SAM columns\n",
    "sam_columns = ['pleasure', 'arousal', 'dominance']\n",
    "\n",
    "# Define the dependent variables\n",
    "dependent_vars = intensity_columns + sam_columns\n",
    "\n",
    "# Define the independent variables continuous independent variables\n",
    "independent_vars = [\n",
    "    'wander_speed', 'wander_roundness', 'wander_cycle_rate', \n",
    "    'blink_temperature', 'blink_slope', 'blink_cycle_rate', \n",
    "    'beep_pitch', 'beep_slope', 'beep_cycle_rate'\n",
    "]\n",
    "\n",
    "# Specify which of the independent variables are continuous\n",
    "independent_continuous_vars = [\n",
    "    'wander_speed', 'wander_roundness', 'wander_cycle_rate', \n",
    "    'blink_temperature', 'blink_cycle_rate', \n",
    "    'beep_pitch', 'beep_cycle_rate'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1307f-a5f9-4d9d-9f71-9e8aaa8b00fb",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "We define a function to detect and remove outliers from the dataset using the Interquartile Range (IQR) method. The function calculates the IQR for each specified column and removes data points that fall below the lower bound or above the upper bound (1.5 times the IQR from the first and third quartiles). We then apply this function to the dependent variables to clean the data. The resulting dataset consists of 2775 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07608f5f-f9c7-406b-9d1b-75bc2fc2dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and remove outliers using IQR\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Remove outliers from the dependent variables\n",
    "cleaned_data = remove_outliers(data, dependent_vars)\n",
    "\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85b5cf-4199-4a6a-9027-99a9cdef5cbf",
   "metadata": {},
   "source": [
    "And we create a dictionary of titles which will be used for plotting figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb3afa-f303-4503-a415-c623b956762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {'wander_speed': 'Wander Speed', 'wander_roundness': 'Wander Roundness', 'wander_roundness_effective': 'Wander Effective Roundness', 'wander_cycle_rate': 'Wander Cycle Rate', \n",
    "          'blink_temperature': 'Blink Temperature', 'blink_slope': 'Blink Slope', 'blink_cycle_rate': 'Blink Cycle Rate', \n",
    "          'beep_pitch': 'Beep Pitch', 'beep_slope': 'Beep Slope', 'beep_cycle_rate': 'Beep Cycle Rate',\n",
    "          'joy_intensity': 'Joy Intensity', 'sadness_intensity': 'Sadness Intensity', 'anger_intensity': 'Anger Intensity', \n",
    "          'fear_intensity': 'Fear Intensity', 'disgust_intensity': 'Disgust Intensity', 'surprise_intensity': 'Surprise Intensity',\n",
    "          'pleasure': 'Pleasure', 'arousal': 'Arousal', 'dominance': 'Dominance'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5701b-3b08-4c4e-8086-f082d7c778f7",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abbb02-459a-48ce-b1ec-42cec69ae888",
   "metadata": {},
   "source": [
    "### Visualize Distributions of Independent Variables\n",
    "\n",
    "To start off the exploratory data analysis, we create a grid of histograms to visualize the distributions of the independent variables. This helps us understand the distribution and spread of each variable. \n",
    "\n",
    "As you can see, the distribution of the independent variables is pretty uniform for the continuous independent variables. This is good, as it will help us ensure that we cover the range of possible values as uniformly as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4641d4f-3175-46d7-b059-0f21e7463799",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(12, 8))\n",
    "\n",
    "for ax, independent_var in zip(axes.flatten(), independent_vars):\n",
    "    # Create the histplot in the specified subplot\n",
    "    sns.histplot(data=cleaned_data, x=independent_var, ax=ax, color='lightskyblue')\n",
    "    ax.set_title(titles[independent_var])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540774a-a1c7-4017-a01f-b2784b2ea722",
   "metadata": {},
   "source": [
    "### Visualize Distributions of Dependent Variables\n",
    "\n",
    "Next, we create a grid of count plots to visualize the distributions of the emotion intensity variables. This helps us understand the distribution of the intensities for each emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c1388-3fbf-419e-a4eb-fbf1bf40dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "for ax, dependent_var in zip(axes.flatten(), intensity_columns):\n",
    "    # Create the catplot in the specified subplot\n",
    "    sns.countplot(data=cleaned_data, x=dependent_var, ax=ax, color='lightskyblue')\n",
    "    ax.set_title(titles[dependent_var])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xlim(-0.5, 5.5)\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(['N/A', 'Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11ef19-59e2-4cf7-837d-f9c05ae44263",
   "metadata": {},
   "source": [
    "Next, we create a grid of count plots to visualize the distributions of the Self-Assessment Manikin columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3a517-10b4-45a5-9280-ca8baf9dba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for ax, dependent_var in zip(axes.flatten(), sam_columns):\n",
    "    # Create the catplot in the specified subplot\n",
    "    sns.countplot(data=cleaned_data, x=dependent_var, ax=ax, color='lightskyblue')\n",
    "    ax.set_title(titles[dependent_var])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticks(range(9))\n",
    "    ax.set_xticklabels(range(1,10))\n",
    "    ax.set_ylabel('Count')\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67537c0-6121-48b0-a05d-f75c87838477",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae5125-4eda-4937-abd6-fb174dee996b",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "We compute the correlation matrix for the independent and dependent variables using Pearson's correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2036a-5a74-4a5f-a6bc-1ca219095595",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = cleaned_data[independent_vars + dependent_vars].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450943b-92a5-44c6-a190-be84a7c4b393",
   "metadata": {},
   "source": [
    "Next, we define a function to perform Pearson correlation tests between each pair of independent and dependent variables. This function also supports multiple testing corrections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776a5961-a56a-47dc-86a7-9b517e73c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_test(df, independent_vars, dependent_vars, alpha=0.05, bonferroni_correction=False, benjamini_hochberg_correction=False):\n",
    "    data = []\n",
    "    p_values_non_zero = []\n",
    "    p_values_positive = []\n",
    "    p_values_negative = []\n",
    "\n",
    "    if bonferroni_correction and benjamini_hochberg_correction:\n",
    "        raise Exception(\"You can only apply one multiple-testing correction.\")\n",
    "    elif bonferroni_correction:\n",
    "        method = 'bonferroni'\n",
    "    elif benjamini_hochberg_correction:\n",
    "        method = 'fdr_bh'\n",
    "    else:\n",
    "        method = None  # No correction\n",
    "\n",
    "    for independent_var in independent_vars:\n",
    "        for dependent_var in dependent_vars:\n",
    "            # Calculate Spearman correlation for two-sided test\n",
    "            rho, p_non_zero = stats.pearsonr(df[independent_var], df[dependent_var], alternative='two-sided')\n",
    "            \n",
    "            # Calculate Spearman correlation for one-sided tests\n",
    "            _, p_negative = stats.pearsonr(df[independent_var], df[dependent_var], alternative='less')\n",
    "            _, p_positive = stats.pearsonr(df[independent_var], df[dependent_var], alternative='greater')\n",
    "\n",
    "            # Collect p-values for later adjustment\n",
    "            p_values_non_zero.append(p_non_zero)\n",
    "            p_values_positive.append(p_positive)\n",
    "            p_values_negative.append(p_negative)\n",
    "            \n",
    "            # Store the initial results\n",
    "            row = {'independent_variable': independent_var, 'dependent_variable': dependent_var, 'correlation': rho, \n",
    "                   'p_non_zero': p_non_zero, 'is_non_zero': p_non_zero < alpha, \n",
    "                   'p_positive': p_positive, 'is_positive': p_positive < alpha / 2, \n",
    "                   'p_negative': p_negative, 'is_negative': p_negative < alpha / 2}\n",
    "            \n",
    "            data.append(row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    correlation_test_results_df = pd.DataFrame(data)\n",
    "    \n",
    "    if method:\n",
    "        # Apply multiple testing correction\n",
    "        correlation_test_results_df['is_non_zero'] = multipletests(p_values_non_zero, alpha=alpha, method=method)[0]\n",
    "        correlation_test_results_df['is_positive'] = multipletests(p_values_positive, alpha=alpha, method=method)[0]\n",
    "        correlation_test_results_df['is_negative'] = multipletests(p_values_negative, alpha=alpha, method=method)[0]\n",
    "\n",
    "    correlation_test_results_df.set_index(['independent_variable', 'dependent_variable'], inplace=True)\n",
    "\n",
    "    return correlation_test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a6985-a4e6-4890-b196-7be0739aabe9",
   "metadata": {},
   "source": [
    "Next, we define a helper function to annotate the significant correlations in the correlation matrix. This function adds asterisks to indicate statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11618b-47a8-48e8-9ad5-cc5633c0508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_annots(correlation_test_results_df):\n",
    "    correlation_test_results_array = correlation_test_results_df.correlation.to_numpy()\n",
    "    annot = correlation_test_results_array.astype(str)\n",
    "    indexed_correlation_test_results_df = correlation_test_results_df.reset_index()\n",
    "    significance_mask = indexed_correlation_test_results_df[indexed_correlation_test_results_df['is_non_zero'] & (indexed_correlation_test_results_df['is_positive'] | indexed_correlation_test_results_df['is_negative'])].index\n",
    "    \n",
    "    # Apply asterisks to significant correlations\n",
    "    for i in range(len(correlation_test_results_array)):\n",
    "        if i in significance_mask:\n",
    "            annot[i] = f'{correlation_test_results_array[i]:.3f}*'\n",
    "        else:\n",
    "            annot[i] = f'{correlation_test_results_array[i]:.3f}'\n",
    "    \n",
    "    annot = annot.reshape(9, 9).T\n",
    "\n",
    "    return annot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa6898-015a-4870-917b-0ffd99ed8274",
   "metadata": {},
   "source": [
    "#### Correlation Matrix of the Independent Variables\n",
    "\n",
    "We then create a heatmap to visualize the correlation matrix of the independent variables. This visualization helps us identify the strength and direction of linear relationships between pairs of variables. The heatmap shows that most independent variables have very low correlation with each other, indicating very low multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d745aa-5364-47ee-b5f1-4859efa9c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_test_results_df = pearson_test(cleaned_data, independent_vars, independent_vars)\n",
    "annot = get_correlation_annots(correlation_test_results_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "ticklabels = [titles[var] for var in independent_vars]\n",
    "sns.heatmap(corr_matrix.iloc[:9, :9], mask=np.triu(np.ones_like(corr_matrix.iloc[:9, :9], dtype=bool), k=1), annot=annot, xticklabels=ticklabels, yticklabels=ticklabels, cmap='coolwarm', fmt='', center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c72aa-5ce9-402a-8ec3-01479cd38b46",
   "metadata": {},
   "source": [
    "#### Correlation Matrix of the Dependent Variables\n",
    "\n",
    "We do the same for the dependent variables. The heatmap shows several meaningful correlations among the dependent variables, indicating that participants understood the assignment of rating the emotions:\n",
    "\n",
    "- **Positive Correlations**:\n",
    "  - **Pleasure and Joy Intensity**: There is a strong positive correlation (0.808*), which makes sense as joy is a high-pleasure emotion.\n",
    "  - **Disgust Intensity and Anger Intensity**: There is a positive correlation (0.509*), indicating that participants often perceive these emotions together.\n",
    "  - **Arousal and Dominance**: There is a positive correlation (0.506*), suggesting that higher arousal is associated with higher feelings of dominance.\n",
    "\n",
    "- **Negative Correlations**:\n",
    "  - **Pleasure and Sadness Intensity**: There is a strong negative correlation (-0.534*), which is expected as sadness is a low-pleasure emotion.\n",
    "  - **Pleasure and Fear Intensity**: There is a negative correlation (-0.450*), indicating that higher fear is associated with lower pleasure.\n",
    "\n",
    "These correlations align with psychological theories of emotions, suggesting that the participants understood and correctly rated the emotions. If we had seen unexpected results, such as a negative correlation between joy and pleasure, it could indicate issues with data quality or participant understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78300d9e-92b3-45a6-83e4-b55abcd024a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_test_results_df = pearson_test(cleaned_data, dependent_vars, dependent_vars)\n",
    "annot = get_correlation_annots(correlation_test_results_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "ticklabels = [titles[var] for var in dependent_vars]\n",
    "sns.heatmap(corr_matrix.iloc[9:, 9:], mask=np.triu(np.ones_like(corr_matrix.iloc[9:, 9:], dtype=bool), k=1), annot=annot, xticklabels=ticklabels, yticklabels=ticklabels, cmap='coolwarm', fmt='', center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112824f2-b8f6-408e-b346-702e1967e2fd",
   "metadata": {},
   "source": [
    "#### Correlation Matrix of the Independent and the Dependent Variables\n",
    "\n",
    "Finally, we do the same for each pair of independent and dependent variables. The heatmap shows that most correlations between the independent and dependent variables are relatively weak, suggesting that the independent variables may not have strong predictive power for the dependent variables if a linear regression model is used. Notable correlations include:\n",
    "\n",
    "- **Arousal and Wander Speed**: There is a positive correlation (0.313*), indicating that higher wander speeds are associated with higher arousal.\n",
    "- **Sadness Intensity and Wander Speed**: There is a negative correlation (-0.148*), suggesting that higher wander speeds are associated with lower sadness intensity.\n",
    "- **Surprise Intensity and Wander Speed**: There is a positive correlation (0.146*), suggesting that higher wander speeds are associated with higher surprise intensity.\n",
    "\n",
    "These weak correlations imply that while the independent variables may not be strong linear predictors, exploring feature engineering and using models that can handle non-linear patterns could potentially improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69eb8f-5d9c-4e5b-9c04-c4b8c89f34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_test_results_df = pearson_test(cleaned_data, independent_vars, dependent_vars)\n",
    "annot = get_correlation_annots(correlation_test_results_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "xticklabels = [titles[var] for var in independent_vars]\n",
    "yticklabels = [titles[var] for var in dependent_vars]\n",
    "sns.heatmap(corr_matrix.iloc[9:, :9], annot=annot, xticklabels=xticklabels, yticklabels=yticklabels, cmap='coolwarm', fmt='', center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab4345-a925-41e7-ab7a-95e2f8cf4df5",
   "metadata": {},
   "source": [
    "### Model Training \n",
    "\n",
    "To predict the dependent variables, we will create regression models using a cross-validation approach. We will use the function `k_folds_training` below to perform K-Fold cross-validation.\n",
    "\n",
    "#### Forced Entry of Variables \n",
    "All models have been trained with forced entry of all independent variables and engineered features. This means that all variables are included in the model without performing any stepwise regression.\n",
    "\n",
    "\n",
    "- **Advantages**:\n",
    "    - Simplicity: This approach simplifies the modeling process as all variables are included without the need for iterative selection procedures.\n",
    "    - Captures Full Feature Set: Ensures that all potential predictors and interactions are considered in the model.\n",
    "- **Disadvantages**:\n",
    "    - Overfitting Risk: Including all variables and interactions increases the complexity of the model, which can lead to overfitting, especially with a large number of features.\n",
    "    - Irrelevant Features: The model may include irrelevant features that do not contribute to predictive power, potentially degrading performance.\n",
    "\n",
    "#### Performance Metrics\n",
    "\n",
    "This function performs 5-fold cross-validation, providing a robust evaluation of model performance by training and testing the model on different subsets of the data. The average Mean Squared Error (`avg_mse_scores`) and average R² (`avg_r2`) values are computed to assess model accuracy and explanatory power in both the training and testing data:\n",
    "\n",
    "- **Train MSE and R²**: These metrics show how well the model fits the training data. High R² and low MSE values indicate good fit.\n",
    "- **Test MSE and R²**: These metrics show how well the model generalizes to unseen data. A significant difference between train and test metrics may indicate overfitting or underfitting.\n",
    "\n",
    "\n",
    "#### Models Used\n",
    "\n",
    "We will use three types of models for the following reasons:\n",
    "\n",
    "1. **Linear Regression Model**:\n",
    "   - Simple and interpretable.\n",
    "   - Good for understanding linear relationships.\n",
    "2. **Random Forest Regressor**:\n",
    "   - Capable of capturing non-linear relationships.\n",
    "   - Robust to overfitting with default settings and parameter tuning.\n",
    "3. **Simple Neural Network**:\n",
    "   - Powerful for capturing complex, non-linear patterns.\n",
    "   - Composed of layers of neurons with activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f7cfe-1db2-4ee0-bf3b-693206de7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds_training(model, X, y, **kwargs):\n",
    "    # Perform K-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    train_mse_scores = []\n",
    "    test_mse_scores = []\n",
    "    train_r2_scores = []\n",
    "    test_r2_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train, **kwargs)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        train_mse_scores.append(mean_squared_error(y_train, y_train_pred, multioutput='raw_values'))\n",
    "        test_mse_scores.append(mean_squared_error(y_test, y_test_pred, multioutput='raw_values'))\n",
    "        \n",
    "        train_r2_scores.append(r2_score(y_train, y_train_pred, multioutput='raw_values'))\n",
    "        test_r2_scores.append(r2_score(y_test, y_test_pred, multioutput='raw_values'))\n",
    "\n",
    "    # Dynamically determine the dependent variables\n",
    "    mask = np.any(np.array([['joy' in column for column in y.columns],\n",
    "                            ['sadness' in column for column in y.columns],\n",
    "                            ['anger' in column for column in y.columns],\n",
    "                            ['fear' in column for column in y.columns],\n",
    "                            ['disgust' in column for column in y.columns],\n",
    "                            ['surprise' in column for column in y.columns],\n",
    "                            ['pleasure' in column for column in y.columns],\n",
    "                            ['arousal' in column for column in y.columns],\n",
    "                            ['dominance' in column for column in y.columns]]), axis=1)\n",
    "                                   \n",
    "    dependent_vars = list(y.columns[mask])\n",
    "    \n",
    "    # Calculate average scores across all folds\n",
    "    avg_train_mse_scores = {dependent_var: round(avg_train_mse_score, 3) for dependent_var, avg_train_mse_score in zip(dependent_vars, np.mean(train_mse_scores, axis=0))}\n",
    "    avg_test_mse_scores = {dependent_var: round(avg_test_mse_score, 3) for dependent_var, avg_test_mse_score in zip(dependent_vars, np.mean(test_mse_scores, axis=0))}\n",
    "    \n",
    "    avg_train_r2_scores = {dependent_var: round(avg_train_r2_score, 3) for dependent_var, avg_train_r2_score in zip(dependent_vars, np.mean(train_r2_scores, axis=0))}\n",
    "    avg_test_r2_scores = {dependent_var: round(avg_test_r2_score, 3) for dependent_var, avg_test_r2_score in zip(dependent_vars, np.mean(test_r2_scores, axis=0))}\n",
    "\n",
    "    return model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2_scores, avg_test_r2_scores\n",
    "\n",
    "def print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2_scores, avg_test_r2_scores):\n",
    "    print(f\"\\nTrain Average Mean Squared Errors:\")\n",
    "    for dependent_var, avg_mse_score in avg_train_mse_scores.items():\n",
    "        print(f\"  -{titles[dependent_var]}: {avg_mse_score}\")\n",
    "    print(f\"Test Average Mean Squared Error:\")\n",
    "    for dependent_var, avg_mse_score in avg_test_mse_scores.items():\n",
    "        print(f\"  -{titles[dependent_var]}: {avg_mse_score}\")\n",
    "\n",
    "    print(f\"\\nTrain Average R-squared Scores:\")\n",
    "    for dependent_var, avg_r2_score in avg_train_r2_scores.items():\n",
    "        print(f\"  -{titles[dependent_var]}: {avg_r2_score}\")\n",
    "    print(f\"Test Average R-squared Scores:\")\n",
    "    for dependent_var, avg_r2_score in avg_test_r2_scores.items():\n",
    "        print(f\"  -{titles[dependent_var]}: {avg_r2_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94267618-8728-4538-bb09-19edf85d6b47",
   "metadata": {},
   "source": [
    "#### Plotting Model Performance: Actual vs. Predicted Arousal\n",
    "\n",
    "Additionally, we define a helper function to visualize the performance of the regression model by plotting actual vs. predicted values for the Arousal variable in both the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cdbab-2df4-4612-a959-4fec5cba6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var_index = {dependent_var: i for i, dependent_var in enumerate(dependent_vars)}\n",
    "\n",
    "def plot_performance(dependent_var, model, X_train, y_train, X_test, y_test):\n",
    "    if '_mean' in dependent_var:\n",
    "        index = dependent_var_index[dependent_var.replace('_mean', '')]\n",
    "    elif '_median' in dependent_var:\n",
    "        index = dependent_var_index[dependent_var.replace('_median', '')]\n",
    "    elif '_mode' in dependent_var:\n",
    "        index = dependent_var_index[dependent_var.replace('_median', '')]\n",
    "    else:\n",
    "        index = dependent_var_index[dependent_var]\n",
    "\n",
    "    if '_intensity' in dependent_var:\n",
    "        var_range = [0, 5]\n",
    "    else:\n",
    "        var_range = [1, 9]\n",
    "       \n",
    "    # Predict on training and test data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # Plot train data\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_train.iloc[:, index], y_train_pred[:, index], color='royalblue', alpha=0.5, label='Train data')\n",
    "    plt.plot(var_range, var_range, 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {titles[dependent_var]}')\n",
    "    plt.xlim(var_range[0] - 0.5, var_range[1] + 0.5)\n",
    "    plt.ylabel(f'Predicted {titles[dependent_var]}')\n",
    "    plt.ylim(var_range[0] - 0.5, var_range[1] + 0.5)\n",
    "    plt.text(var_range[0], var_range[1] - 0.5, \"MSE = {:.3f}\".format(mean_squared_error(y_train, y_train_pred)))\n",
    "    plt.text(var_range[0], var_range[1] - 1, \"R² = {:.3f}\".format(r2_score(y_train, y_train_pred)))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Plot test data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test.iloc[:, index], y_test_pred[:, index], color='lightskyblue', alpha=0.5, label='Test data')\n",
    "    plt.plot(var_range, var_range, 'k--', lw=2)\n",
    "    plt.xlabel(f'Actual {titles[dependent_var]}')\n",
    "    plt.xlim(var_range[0] - 0.5, var_range[1] + 0.5)\n",
    "    plt.ylabel(f'Predicted {titles[dependent_var]}')\n",
    "    plt.ylim(var_range[0] - 0.5, var_range[1] + 0.5)\n",
    "    plt.text(var_range[0], var_range[1] - 0.5, \"MSE = {:.3f}\".format(mean_squared_error(y_test, y_test_pred)))\n",
    "    plt.text(var_range[0], var_range[1] - 1, \"R² = {:.3f}\".format(r2_score(y_test, y_test_pred)))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4c4d4-d124-4ec3-b148-8597ea62331c",
   "metadata": {},
   "source": [
    "#### Data Preparation for Regression Models\n",
    "\n",
    "Before creating and evaluating regression models, we prepare the data by selecting the variables, standardizing the features, and splitting the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a1fd2-1f85-4b9f-963e-19d60305b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the independent and dependent variables\n",
    "X = cleaned_data[independent_vars]\n",
    "y = cleaned_data[dependent_vars]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d20b-24db-4e74-9a31-d5d966ec1a75",
   "metadata": {},
   "source": [
    "#### Target Dependent Variable\n",
    "\n",
    "We then define which dependent variable we will use to evaluate the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2d4aa-67a9-45b9-9b16-1b4aa4933826",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dependent_var = 'arousal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7dab9-b94c-4a7e-bc23-87d952a54405",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Forced Entry of all Independent Variables\n",
    "\n",
    "We build and train a linear regression model with all datapoints using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a02e3f-62fc-4fcd-be1f-1800e6dc7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee7f48-3dd9-4313-8929-22d4a1f76a2b",
   "metadata": {},
   "source": [
    "- **Consistency**: The MSE values for both train and test sets are quite similar, indicating that the model does not suffer from significant overfitting.\n",
    "- **Low R-squared**: Both the train (0.047) and test (0.038) R-squared values are very low, suggesting that the linear model explains only a small fraction of the variance in the dependent variables.\n",
    "- **Predictive Power**: The low R-squared values and relatively high MSEs imply that the linear regression model has limited predictive power for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fa62e-1844-4a59-8fad-b4adf2e49b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec64064-9c6a-491e-9af4-722bb78a532d",
   "metadata": {},
   "source": [
    "Below we include scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the linear regression model. The key observations are the following:\n",
    "\n",
    "- **Fit**: The model exhibits high bias, as evidenced by its inability to capture the variability in the actual values. The predictions are overly simplistic and do not vary much across different actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8300b-d55d-4677-b79c-0fe828a55812",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b8317-13e0-4891-9b56-f6bf5b86b2fb",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model with Forced Entry of all Independent Variables\n",
    "\n",
    "Since the linear regression model underfitted the data, we build and train a random forest regression model with all datapoints using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5c051-74df-40c8-be20-33ba9b87699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cae66-a5bf-4576-9955-65e7e19ca63c",
   "metadata": {},
   "source": [
    "- **Overfitting**: The model performs significantly better on the training set (Train R-squared: 0.302) compared to the test set (Test R-squared: -0.15), indicating overfitting.\n",
    "- **Train vs. Test MSE**: \n",
    "  - **Train MSE**: Lower MSE values for all dependent variables on the training set indicate good fit to the training data.\n",
    "  - **Test MSE**: Higher MSE values on the test set suggest that the model does not generalize well to unseen data.\n",
    "- **Predictive Power**: The negative R-squared value for the test set (-0.15) implies that the model performs worse than a horizontal line (mean of the data), further highlighting poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5342b-b6bd-40b4-8b73-bffdf5b18966",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca38bf2-9fa7-4aee-a955-6de481b549e9",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the random forest regression model. The key observations are the following:\n",
    "\n",
    "- **Fit**: The training plot suggests low bias (good fit to training data) but high variance (overfitting). The test plot suggests that this variance leads to poor generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff71a01-ed12-4909-83af-20bec42b67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80610af6-89f0-49a8-93ff-c7e2a06c96dd",
   "metadata": {},
   "source": [
    "#### Neural Network Regression Model with Forced Entry of all Independent Variables\n",
    "\n",
    "Finally, we also build and train a neural network model with all datapoints using the `k_folds_training` function to perform cross-validation and evaluate its performance. This neural network model includes two hidden layers with ReLU activation and an output layer with a linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c419c6-afce-478b-a60d-2b08623035d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f9075-3a45-4f6c-aa44-3216d38d5a88",
   "metadata": {},
   "source": [
    "- **Moderate Improvement**: The neural network shows moderate improvement in performance compared to the linear regression model, as indicated by a higher Train R-squared (0.128) and Test R-squared (0.071).\n",
    "- **Train vs. Test MSE**: The MSE values for both train and test sets are closer to each other, suggesting that the neural network has better generalization compared to the Random Forest model. However, there is still room for improvement.\n",
    "- **Predictive Power**: The higher R-squared values compared to the linear regression model indicate that the neural network captures more of the variance in the dependent variables, although the values are still relatively low, implying limited predictive power.\n",
    "- **Overfitting**: The difference between train and test R-squared values is smaller compared to the Random Forest model, indicating less overfitting.\n",
    "\n",
    "Overall, the neural network model performs better than the linear regression model and shows less overfitting compared to the random forest model. However, the relatively low R-squared values suggest that there is still significant room for improvement in capturing the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903ade8-32d6-400c-b150-d4d5c5fbd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe96711-8886-47c3-9d6c-9e89acb07db1",
   "metadata": {},
   "source": [
    "Finally, the image shows scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the neural network model:\n",
    "\n",
    "- **Fit**: The neural network model shows a marginally improved fit to the training data compared to the linear regression and random forest models. Still, the points are not closely aligned with the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719db6ad-2e72-4882-9e73-fbf4b3209bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, nn_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1a25b-8a13-4886-8189-f6ff1f24605d",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1482709-a6bb-4aac-a103-6cbce5b93d86",
   "metadata": {},
   "source": [
    "### Engineering Polynomial Features with Interaction Effects\n",
    "\n",
    "To capture interaction effects and improve the predictive power of our models, we engineered polynomial features. This process creates new features that represent all possible combinations of the original features up to a specified degree. We used the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f7930-7668-4526-baca-ba3e61e18a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_poly_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd13568-af32-4287-92c5-a84b942933cd",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Forced Entry of all Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a linear regression model with all independent variables and the engineered polynomial features using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fb799-af15-4cba-84c9-225ed7be52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f40ed-8cdf-4296-af51-2e964ea45e08",
   "metadata": {},
   "source": [
    "- **Improved Fit with Polynomial Features**:\n",
    "  - The introduction of polynomial features with interaction effects resulted in a slight improvement in both train and test R-squared values compared to the simple linear regression model.\n",
    "  - The R-squared values remain relatively low (Train: 0.097, Test: 0.052), indicating that the model still captures a limited portion of the variance in the dependent variables.\n",
    "- **Train vs. Test Performance**:\n",
    "  - The MSE values for both train and test sets are closer to each other, suggesting that the model generalizes better compared to the models that did not make use of the new features.\n",
    "- **Effect of Forced Entry**:\n",
    "  - Despite the potential for overfitting, the relatively low R-squared values suggest that the model's complexity did not lead to significant overfitting, but it also did not substantially improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a1174-0527-451a-9ecc-d51480b76b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff60a38-8478-4696-a428-b6578bbba6ce",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the linear regression model with polynomial features.\n",
    "\n",
    "- **Fit**: The linear regression model with polynomial features does not shows an improved fit to both the training and test data compared to the simple linear regression model. The points are still not closely aligned with the diagonal line, which ideally would indicate better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcece2-de71-438a-baf8-04bb50f54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa098d0b-7254-4c52-9b89-dfb713f4840b",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model with Forced Entry of all Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a random forest regression model with all independent variables and the engineered polynomial features using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4cc1c-820d-4cbd-90ac-7059c90464db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070104f-5b03-4603-973b-78f46639f111",
   "metadata": {},
   "source": [
    "- **No Improvement with Polynomial Features**:\n",
    "  - The introduction of polynomial features did not improve the Random Forest model's performance. The R-squared values are identical to those obtained without polynomial features (Train: 0.302, Test: -0.15).\n",
    "  - The lack of improvement suggests that the Random Forest model is already capable of capturing non-linear relationships and interaction effects without the need for polynomial features.\n",
    "- **Overfitting**:\n",
    "  - The model performs significantly better on the training set compared to the test set, indicating overfitting. The high train R-squared (0.302) and low test R-squared (-0.15) highlight this issue.\n",
    "  - Overfitting is further evidenced by the lower MSE values on the training set and higher MSE values on the test set.\n",
    "- **Training Performance**:\n",
    "  - The model fits the training data well, as indicated by lower MSE values and a higher R-squared value.\n",
    "- **Test Performance**:\n",
    "  - The model struggles to generalize to the test data, with higher MSE values and a negative R-squared value, indicating poor predictive performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26bb28-61bd-4b9f-97f8-2dcbd5328396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6c74-8c0d-439d-90d4-c3668a2f5e7a",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the linear regression model with polynomial features.\n",
    "\n",
    "- **Fit**: this model does not fit the data well, although it seems that it could potentially fit the data better if fine-tuning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d41f98-4c79-46c8-b03d-6fd5d1660bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a6499-999c-4126-a02e-809ef04cb996",
   "metadata": {},
   "source": [
    "#### Neural Network Model with Forced Entry of All Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We built and trained a neural network model with all independent variables and the engineered polynomial features using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e942f-bf4d-4c87-a282-0c3e379dfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_poly_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_poly_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d4f91-d80b-457c-bda5-4fe64468f771",
   "metadata": {},
   "source": [
    "- **Improved Fit with Polynomial Features**:\n",
    "  - The neural network model with polynomial features shows improved performance compared to the neural network without polynomial features, as indicated by higher train and test R-squared values.\n",
    "  - The R-squared values (Train: 0.174, Test: 0.055) indicate that the model captures more variance in the dependent variables compared to the simple neural network model.\n",
    "- **Train vs. Test Performance**:\n",
    "  - The MSE values for both train and test sets are relatively close, suggesting that the model generalizes better than the random Forest model.\n",
    "  - The smaller difference between train and test R-squared values indicates that the model does not suffer significantly from overfitting.\n",
    "- **Predictive Power**:\n",
    "  - The neural network with polynomial features demonstrates better predictive power than the linear regression and Random Forest models, capturing more complex relationships and interactions in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fa63b-f74f-4993-b97d-9d909fdc5be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f71af-baca-4102-8a34-557d467f28b8",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of Arousal for both the training and test datasets using the neural network model with polynomial features.\n",
    "\n",
    "- **Fit**: as is visible, the model does not really learn how to predict the dependent variables effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26166ea2-71eb-4357-bdf6-5c080df04b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, nn_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aae282-f252-4cfb-8c7b-fbaaaa023c98",
   "metadata": {},
   "source": [
    "## Reducing Noise in the Dependent Variables by Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918c7e9-09ed-43e6-b788-ec81e776f617",
   "metadata": {},
   "source": [
    "Given the inherent noisiness of the dependent variables, we decided to group the dependent variables by `video_id` and aggregate them to find the mean, median, and mode of all dependent variables across videos. This approach aims to simplify the dependent variables and reduce noise, potentially leading to better regression models. Essentially, the models would learn to predict the mean, median, or mode of the dependent variables.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Reduced Data Points**: Since we only have 512 unique videos, aggregating the dependent variables by `video_id` results in only 512 data points. This is a significant reduction in the dataset size.\n",
    "- **Impact on Model Performance**: The reduced dataset size may impact the performance of complex models, such as neural networks, which typically require larger datasets to perform well. This limitation should be considered when evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb54fb-a93c-42c2-9b30-5e86bf0ef835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the dependent variables by video_id (mean, median, and mode)\n",
    "dependent_aggregated_mean = cleaned_data.groupby('video_id')[dependent_vars].mean().add_suffix('_mean')\n",
    "dependent_aggregated_median = cleaned_data.groupby('video_id')[dependent_vars].median().add_suffix('_median')\n",
    "dependent_aggregated_mode = cleaned_data.groupby('video_id')[dependent_vars].agg(lambda x: x.mode().iloc[0]).add_suffix('_mode')\n",
    "\n",
    "# Add the titles of the aggregated variables\n",
    "for dependent_var in dependent_vars:\n",
    "    for aggregation in ['mean', 'median', 'mode']:\n",
    "        dependent_aggregated_var = dependent_var + '_' + aggregation\n",
    "        titles[dependent_aggregated_var] =  aggregation.title() + ' ' + titles[dependent_var]\n",
    "\n",
    "# Merge the aggregated dependent variables with the original independent variables\n",
    "aggregated_dependent_vars = pd.concat([dependent_aggregated_mean, dependent_aggregated_median, dependent_aggregated_mode], axis=1)\n",
    "cleaned_data = cleaned_data.merge(aggregated_dependent_vars, on='video_id')\n",
    "cleaned_aggregated_data = cleaned_data.reset_index().drop_duplicates('video_id').set_index('video_id')\n",
    "cleaned_aggregated_data = cleaned_aggregated_data.drop(['start_time', 'end_time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33c47f-f2cf-47f4-9428-4e5921a7aa10",
   "metadata": {},
   "source": [
    "We confirm that we indeed have 512 data points now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f50d94-8560-400e-97d8-6209312325c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_aggregated_data.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda68a4-72de-402b-b204-8dec85ab7a4d",
   "metadata": {},
   "source": [
    "### Target Aggregation\n",
    "\n",
    "Next, we will select the target aggregation for the dependent variables based on how much they actually reduce the noise of the dependent variables. For this, we calculate the descriptive statistics (mean, median, standard deviation) for each of the three aggregated datasets (mean, median, mode) to compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7ba6d-ac84-4310-81bc-78a58cd758ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_aggregated_mean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f467b0b-9266-40df-ac96-b5f1501edff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_aggregated_median.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bef075-1266-4659-94c5-508c0b530a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_aggregated_mode.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c493c6a-1ccd-47ac-ab12-d29cd22870cc",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- **Mean Aggregation**: results in higher mean values and lower standard deviations for most dependent variables, indicating less variability.\n",
    "- **Median Aggregation**: shows slightly lower means compared to the mean aggregation but higher standard deviations, indicating moderate variability.\n",
    "- **Mode Aggregation**: results in the lowest mean values and the highest standard deviations, indicating the highest variability.\n",
    "\n",
    "The mean aggregation method appears to reduce noise effectively, resulting in less variability in the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fc579-1ea0-4e02-9198-75eb9ba7049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select mean because it's the one that reduces noise the most\n",
    "target_aggregation = 'mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1baa3-3a83-4e3a-888d-61097c7a903c",
   "metadata": {},
   "source": [
    "### Correlation Matrix Analysis with Aggregated Dependent Variables\n",
    "\n",
    "Next, we can recompute the correlation matrix to see if the aggregation results in a larger effect size between each pair of independent and aggregated dependent variables.\n",
    "\n",
    "The effect size of the correlations appears to have increased compared to the original data, indicating stronger relationships between the variables when considering aggregated values. Notable correlations include:\n",
    "\n",
    "- **Mean Arousal and Wander Speed**: There is a strong positive correlation (0.552*), indicating that higher wander speeds are associated with higher arousal levels. This correlation is significantly stronger than in the original data.\n",
    "- **Mean Sadness Intensity and Wander Speed**: There is a moderate negative correlation (-0.279*), suggesting that higher wander speeds are associated with lower sadness intensity.\n",
    "- **Mean Surprise Intensity and Wander Speed**: There is a positive correlation (0.278*), indicating that higher wander speeds are associated with higher surprise intensity.\n",
    "- **Mean Anger Intensity and Blink Temperature**: There is a moderate positive correlation (0.220*), suggesting that higher blink temperatures are associated with higher anger intensity.\n",
    "- **Mean Fear Intensity and Beep Pitch**: There is a moderate positive correlation (0.197*), indicating that higher beep pitches are associated with higher fear intensity.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- **Increased Effect Sizes**: The correlations between the independent and dependent variables have increased in magnitude after aggregation, suggesting that aggregating the dependent variables by video_id has reduced noise and revealed stronger underlying relationships.\n",
    "- **Predictive Potential**: The stronger correlations imply that the independent variables may have more predictive power for the aggregated dependent variables, potentially leading to better regression models.\n",
    "- **Key Relationships**: The notable correlations, such as between wander speed and mean arousal, highlight key relationships that can be explored further for predictive modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca038-fd68-4698-a4fa-0fee119409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target aggregated variable\n",
    "mask = [target_aggregation in column for column in cleaned_aggregated_data.columns]\n",
    "dependent_aggregated_vars = list(cleaned_aggregated_data.columns[mask])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = cleaned_aggregated_data[independent_vars + dependent_aggregated_vars].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23f58b-1815-4d5c-a31b-4f2e96876cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_test_results_df = pearson_test(cleaned_aggregated_data, independent_vars, dependent_aggregated_vars)\n",
    "annot = get_correlation_annots(correlation_test_results_df)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "xticklabels = [titles[var] for var in independent_vars]\n",
    "yticklabels = [titles[var] for var in dependent_aggregated_vars]\n",
    "sns.heatmap(corr_matrix.iloc[9:, :9], annot=annot, xticklabels=xticklabels, yticklabels=yticklabels, cmap='coolwarm', fmt='', center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0904e5-99c3-42ac-916e-177ff5d85cf1",
   "metadata": {},
   "source": [
    "To visualize these stronger effect sizes, we create a few scatter plots of pairs of independent variables and  aggregated dependent variable that have the highest effect size per modality.\n",
    "\n",
    "Particularly in the Wander Speed vs. Mean Arousal plot, we can observe that this pair has a stronger linear correlation than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c4470a-4b3a-400d-b4a6-2d768784f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Scatter plot for Wander Speed vs Arousal\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(data=cleaned_aggregated_data, x='wander_speed', y='_'.join(('arousal', target_aggregation)), color='lightskyblue')\n",
    "plt.title('Wander Speed vs Mean Arousal')\n",
    "plt.xlabel('Wander Speed')\n",
    "plt.ylabel('Mean Arousal')\n",
    "plt.ylim(1, 9)\n",
    "\n",
    "# Scatter plot for Blink Temperature vs Anger Intensity\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(data=cleaned_aggregated_data, x='blink_temperature', y='_'.join(('anger_intensity', target_aggregation)), color='lightskyblue')\n",
    "plt.title('Blink Temperature vs. Mean Anger Intensity')\n",
    "plt.xlabel('Blink Temperature')\n",
    "plt.ylabel('Mean Anger Intensity')\n",
    "plt.ylim(0, 5)\n",
    "plt.yticks(range(0, 6), ['N/A', 'Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "# Scatter plot for Beep Pitch vs Surprise Intensity\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.scatterplot(data=cleaned_aggregated_data, x='beep_pitch', y='_'.join(('surprise_intensity', target_aggregation)), color='lightskyblue')\n",
    "plt.title('Beep Pitch vs Mean Surprise Intensity')\n",
    "plt.xlabel('Beep Pitch')\n",
    "plt.ylabel('Mean Surprise Intensity')\n",
    "plt.ylim(0, 5)\n",
    "plt.yticks(range(0, 6), ['N/A', 'Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8325384-7890-4274-839a-a5f5bead47b3",
   "metadata": {},
   "source": [
    "### Model Training with Aggregated Dependent Variables\n",
    "\n",
    "Next, we repeat the process of training different models, but this time using the new aggregated dependent variables as our training labels. Again, we will only consider models trained with force entry of all independent variables and any engineered features.\n",
    "\n",
    "#### Data Preparation for Regression Models\n",
    "\n",
    "Before creating and evaluating regression models, we prepare the data by selecting the variables, standardizing the features, and splitting the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56dc46f-275d-4f73-a244-f513f8c05be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the independent and dependent variables\n",
    "X = cleaned_aggregated_data[independent_vars]\n",
    "y = cleaned_aggregated_data[dependent_aggregated_vars]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151512e9-80a6-410a-9e9c-aad695776b09",
   "metadata": {},
   "source": [
    "#### Linear Regression Model of the Agreggated Variables with Forced Entry of All Independent Variables\n",
    "\n",
    "We then build and train a linear regression model with all independent variables and the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e431db-83b7-4812-b2c8-4088c7650490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b2d4d-f8a7-45c1-9ddb-957be70f1d14",
   "metadata": {},
   "source": [
    "- **Improved Fit with Aggregated Data**:\n",
    "  - The linear regression model shows improved performance compared to the initial models with the original noisy dependent variables. The R-squared values are higher for both the training (0.179) and test (0.128) sets.\n",
    "  - The lower MSE values for both the training and test sets indicate that the model fits the aggregated data better.\n",
    "- **Train vs. Test Performance**:\n",
    "  - The relatively close MSE values for the training and test sets suggest that the model generalizes better with the aggregated data compared to the original data.\n",
    "  - The improved R-squared values indicate that the model captures more variance in the aggregated dependent variables.\n",
    "- **Predictive Power**:\n",
    "  - The linear regression model demonstrates better predictive power with the aggregated data, capturing more of the underlying relationships between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fa10b-cac1-4abb-95f1-8195f739b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d4389-d0f6-4d76-b8b2-9c671924ed47",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of the Mean Arousal for both the training and test datasets using the linear model that predicts the aggregated dependent variables.\n",
    "\n",
    "- **Fit**: The alignment of points with the diagonal line in the training plot indicates that the model captures the underlying patterns in the training data to a reasonable extent. Still, the spread of points around the diagonal line suggests that while the model fits the data moderately well, there is still some variability that it does not capture perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719564d7-ebbe-4e99-85a8-0f6ab6d78e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9c2ce-ebb2-48cd-add6-bccc345dc749",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model of the Agreggated Variables with Forced Entry of All Independent Variables\n",
    "\n",
    "Next, we build and train a random forest regression model with all independent variables and the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16459e0c-b105-4bea-8d1d-6cf9fba4ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58be953-064b-45df-bb7c-9d085fa6da01",
   "metadata": {},
   "source": [
    "- **Excellent Fit on Training Data**:\n",
    "  - The Random Forest model shows a very low MSE and high R-squared (0.877) on the training data, indicating an excellent fit. The model captures the variability in the training data almost perfectly.\n",
    "- **Test Performance**:\n",
    "  - The performance on the test data is significantly worse compared to the training data, with higher MSE and a lower R-squared (0.146). This indicates that the model is overfitting the training data and does not generalize well to unseen data.\n",
    "- **Overfitting**:\n",
    "  - The substantial difference between the train and test R-squared values highlights the overfitting issue. While the model performs exceptionally well on the training set, it struggles to maintain this performance on the test set.\n",
    "  - The test R-squared value of 0.146 suggests that the model captures some variability in the test data, but not enough to be considered highly predictive.\n",
    "- **Predictive Power**:\n",
    "  - Despite the overfitting, the Random Forest model shows some degree of predictive power on the test set, as indicated by the test R-squared value. The aggregated data has helped in capturing some underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74802638-e2da-4745-a0ff-3ea97ecfd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0ca16-e4a6-4ea6-88f4-2fde56ea5937",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of the Mean Arousal for both the training and test datasets using the random forest regression model that predicts the aggregated dependent variables.\n",
    "\n",
    "- **Fit**: The tight clustering of points along the diagonal line in the training plot indicates that the Random Forest model captures the underlying patterns in the training data exceptionally well. The test plot also shows a good spread around the diagonal line, indicating that the model's performance is also good on the test data for the mean arousal. However, this might not be the case for other aggregated dependent variables. This is further explaine by the higher MSE values and lower R-squared for the test data suggest that the model does not generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc3e1c-d33d-4212-a2e6-2a669c517918",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704abf81-950f-4b24-b6fe-c23e3cc74e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5318d1-cc57-4bba-ae70-43f8d90cf90a",
   "metadata": {},
   "source": [
    "#### Neural Network Model of the Agreggated Variables with Forced Entry of All Independent Variables\n",
    "\n",
    "Finally, we build and train a neural network model with all independent variables and the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bf5d2-8964-4bd9-b832-6e3ef74fd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c97c9e-4fcf-4eb5-abd7-bace85d27116",
   "metadata": {},
   "source": [
    "- **Moderate Fit on Training Data**:\n",
    "  - The neural network model shows moderate MSE values and a train R-squared of 0.298, indicating a reasonable fit to the training data.\n",
    "  - The model captures a substantial portion of the variability in the training data, but not as well as the random forest model.\n",
    "- **Test Performance**:\n",
    "  - The performance on the test data is slightly worse than on the training data, with higher MSE and a lower R-squared of 0.139.\n",
    "  - The moderate test performance indicates that the model generalizes reasonably well but not perfectly to unseen data.\n",
    "- **Overfitting**:\n",
    "  - The difference between the train and test R-squared values is smaller than for the random forest model, indicating less overfitting.\n",
    "  - The neural network model maintains better generalization compared to the random forest model, though there is still some drop in performance from train to test data.\n",
    "- **Predictive Power**:\n",
    "  - The neural network model demonstrates moderate predictive power, capturing important patterns in the data.\n",
    "  - The aggregated data has likely contributed to the improved performance, but further tuning could enhance the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb9a38-1203-4fb3-a888-f528c8ac7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca954de7-2c88-430f-85b4-07b9c0ba939c",
   "metadata": {},
   "source": [
    "The image below shows scatter plots comparing the actual and predicted values of the Mean Arousal for both the training and test datasets using the neural network model that predicts the aggregated dependent variables.\n",
    "\n",
    "- **Fit**: The reasonable alignment of points along the diagonal line in both plots suggests that the neural network model performs moderately well, capturing key relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be9008-d45a-412b-948d-c5ca7f12e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, nn_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c134ea-5c4a-4e59-9fdd-ae0b0cb04120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_poly_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a46b46-9cd1-47ee-a89f-a98a6d21832c",
   "metadata": {},
   "source": [
    "#### Linear Regression Model of the Agreggated Variables with Forced Entry of All Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a linear regression model with all independent variables with the polynomial features with the interaction effects to predict the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b4a7b-545b-4edd-813e-0318e6341f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba454b-217c-4373-863a-baf29fecdcdb",
   "metadata": {},
   "source": [
    "- **Training Data**: Moderate fit with an R-squared of 0.372, indicating the model captures key patterns.\n",
    "- **Test Data**: Poor generalization with an R-squared of 0.149, and there is some overfitting.\n",
    "- **Predictive Power**: Improved due to polynomial features and interaction effects, capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c42fd-f335-4933-82ee-892f7d721e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a375841-212d-4026-a57e-6dd343ef83e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e431727-230b-4f87-8245-7e3bb7ad0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5473b7-18db-4004-be0a-397c9e54de52",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model of the Agreggated Variables with Forced Entry of All Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a random forest regression model with all independent variables with the polynomial features with the interaction effects to predict the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964d65d-8327-4934-9fc8-92bb6da35b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abe135-3f81-4a79-92f1-80d6e1718e36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836344c5-d485-4b7d-baf2-ccaa7bec9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d4669-dd8f-4420-aa2a-ff840637f5e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50af21-e7c6-4461-a5a8-bfe7f2664aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6dfa1-1dda-490f-99e7-27d4edebe07e",
   "metadata": {},
   "source": [
    "#### Neural Network Model of the Agreggated Variables with Forced Entry of All Independent Variables and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a neural network model with all independent variables with the polynomial features with the interaction effects to predict the aggregated dependent variables using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc9c46-4e2b-4ccb-8c30-349c320ead42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_poly_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_poly_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2baf76d-a41e-4551-b95e-22a109f710c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc706c8a-7542-420a-b94b-35c0b78bfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd37de8-b220-44a2-84ee-5b87d00e30f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259a6b7-f750-4855-afea-9446ed9274f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var + '_' + target_aggregation, nn_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc946ed5-adc5-4979-8938-ae338739d96d",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606320b-df51-4d31-bbf2-e70e5ac19d4e",
   "metadata": {},
   "source": [
    "## Clustering with Gaussian Mixture Model and using the Membership Probabilities as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6df11-f286-494a-9744-f3d1ecd16543",
   "metadata": {},
   "source": [
    "The Gaussian Mixture Model is used to cluster the data points into different groups based on the distribution of the features. It allows us to identify underlying patterns in the data that can be useful for improving model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc057d-22d8-4046-9234-797f8cbf4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cleaned_data[dependent_vars])\n",
    "\n",
    "# Determine the optimal number of components using BIC and AIC\n",
    "n_components_range = range(1, 10)\n",
    "bics = []\n",
    "aics = []\n",
    "\n",
    "for n in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "    gmm.fit(data_scaled)\n",
    "    bics.append(gmm.bic(data_scaled))\n",
    "    aics.append(gmm.aic(data_scaled))\n",
    "\n",
    "# Plot the BIC and AIC scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_components_range, bics, label='BIC')\n",
    "plt.plot(n_components_range, aics, label='AIC')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC / AIC Score')\n",
    "plt.legend()\n",
    "plt.title('BIC and AIC Scores for GMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e54b90-bf40-4984-92db-1ac2d7ba820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "gmm.fit(data_scaled)\n",
    "probabilities = gmm.predict_proba(data_scaled)\n",
    "\n",
    "# Add cluster probabilities as new columns\n",
    "probabilities_df = pd.DataFrame(probabilities, columns=[f'cluster_prob_{i}' for i in range(probabilities.shape[1])], index=cleaned_data.index)\n",
    "\n",
    "cleaned_data = pd.concat([cleaned_data, probabilities_df], axis=1)\n",
    "\n",
    "cluster_cols = list(probabilities_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd40dd-b2ad-4429-9aae-a27520cdf6e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a939b-1efb-4575-87ed-440ab876757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['cluster_id'] = np.argmax(cleaned_data[cluster_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07ffd4-9dff-4f9e-931a-6cf0fd6d3ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "for ax, dependent_var in zip(axes.flatten(), intensity_columns):\n",
    "    # Create the catplot in the specified subplot\n",
    "    sns.kdeplot(data=cleaned_data, x=dependent_var, ax=ax, hue='cluster_id', fill=True, palette=\"crest\", alpha=.5, warn_singular=False)\n",
    "    ax.set_title(titles[dependent_var])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(['N/A', 'Very Low', 'Low', 'Average', 'High', 'Very High'])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f2e91-afba-4047-b7bc-a359759c0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, dependent_var in zip(axes.flatten(), sam_columns):\n",
    "    # Create the catplot in the specified subplot\n",
    "    sns.kdeplot(data=cleaned_data, x=dependent_var, ax=ax, hue='cluster_id', fill=True, palette=\"crest\", alpha=.5)\n",
    "    ax.set_title(titles[dependent_var])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticks(range(1,10))\n",
    "    \n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aa92f-bd78-4e27-ac35-0fc16980483e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ea0461-7b3e-4db9-8644-dc3c8045c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the independent and dependent variables\n",
    "X = cleaned_data[independent_vars + cluster_cols]\n",
    "y = cleaned_data[dependent_vars]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394ab0c-fb91-46f3-a6fa-6bd83a6ad88c",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Forced Entry of All Independent Variables and the Cluster Membership Probabilities\n",
    "\n",
    "We then build and train a linear regression model with all independent variables with the cluster membership probabilites using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fffdb-f092-4ed9-ba57-8d027f5484d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef59ac1-4e5c-45be-b968-d6570f8727b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44982c1a-68a0-4da6-b89e-de3502b8e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49f055-98c9-4e46-94df-4f054f266c71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817efcd-f4c5-48a8-80e1-5a61ea429ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85a2bf-d1da-4195-9fbc-2d0fdc839f58",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model with Forced Entry of All Independent Variables and the Cluster Membership Probabilities\n",
    "\n",
    "We then build and train a random forest regression model with all independent variables with the cluster membership probabilites using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02cadd-9c5c-45ac-aede-87f4edd6366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4275fe-1f48-42cc-80cf-24329c50a37e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba807c8-882d-409d-82c8-2563a2b19fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2fb96-f01c-4832-951c-86d7ebb0443e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b0833-86a1-454a-8c25-9f22aedd78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97d6e2-56e8-4247-be96-9e6d7eeeb353",
   "metadata": {},
   "source": [
    "#### Neural Network Model with Forced Entry of All Independent Variables and the Cluster Membership Probabilities\n",
    "\n",
    "We then build and train a neural network model with all independent variables with the cluster membership probabilites using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f75c46-a40c-45e7-b402-403812a80480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e1e96d-6348-4129-8597-67b795550f55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8def294-72f2-47b7-a741-d3586e99813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c06229-b5d1-4511-b888-08a07b5891ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985a2ecf-0412-4a86-a55d-a1ddbaa34c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, nn_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d9f61-b75e-4ea8-9e0a-d53297546c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features of degree 2\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_poly_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad74f9-75c3-463a-bf09-87aecfe24a74",
   "metadata": {},
   "source": [
    "#### Linear Regression Model with Forced Entry of All Independent Variables, the Cluster Membership Probabilities and Polynomial Features with Interaction Effects\n",
    "\n",
    "We then build and train a linear regression model with all independent variables with the cluster membership probabilites and the polynomial interaction effects using the `k_folds_training` function to perform cross-validation and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1284e67-97dc-4bb2-9835-20974890c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(linear_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66455c-0144-4152-9fba-83c766265e32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fcaba-f836-429a-b1fa-384e4b5d0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b602aea-f20e-4d0d-a610-71b9ed836145",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892ecf0-2e62-44d1-b416-24ec57a24bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, linear_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4c2d4-e5fe-4ab5-9fb2-e6af8b46b1b0",
   "metadata": {},
   "source": [
    "#### Random Forest Regression Model with Forced Entry of All Independent Variables, the Cluster Membership Probabilities and Polynomial Features with Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45a8ecc-5ca1-4460-b5c9-fa4e9fc4cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(rf_model, X_poly_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2e025-fef5-4c66-b1c2-f24e637adb63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5bfbd-c9a9-4592-9e49-8fc855226ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd11b3-3af1-468d-ba81-0979e62c123d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647158fc-3b5a-4b73-bec6-cb80dc369753",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, rf_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0a2dd-1665-42b4-a68f-e9d3569d8de4",
   "metadata": {},
   "source": [
    "#### Neural Network Model with Forced Entry of All Independent Variables, the Cluster Membership Probabilities and Polynomial Features with Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7bd86-51e0-47e5-8f12-af2a23841ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_poly_scaled.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(y.shape[1], activation='linear'))\n",
    "nn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "nn_model, avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2 = k_folds_training(nn_model, X_poly_scaled, y, epochs=50, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c86b12-a457-4971-ac8b-52add7962fdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4df5d-bb5e-432d-a930-532211502422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(avg_train_mse_scores, avg_test_mse_scores, avg_train_r2, avg_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a2105-61cc-4d75-980b-809f0d64e251",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c341fb9-756f-4c0c-af35-51b3691af4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(target_dependent_var, nn_model, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
